{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fractal Time Series Compression: Interactive Demo\n",
    "\n",
    "This notebook demonstrates three fractal-based compression methods for time series data:\n",
    "\n",
    "1. **Iterated Function Systems (IFS)** - Uses contractive transformations\n",
    "2. **Fractal Coding** - Block-based compression with self-similarity\n",
    "3. **Fractal Interpolation** - Advanced decompression using FIFs\n",
    "\n",
    "We'll explore different types of time series and compare compression performance with detailed visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from typing import Dict, Tuple, List\n",
    "import time\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import our fractal compression modules\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('./src')\n",
    "\n",
    "from data.generator import TimeSeriesGenerator\n",
    "from data.loader import TimeSeriesLoader\n",
    "from compression.ifs_compression import IFSCompressor\n",
    "from compression.fractal_coding import FractalCodingCompressor\n",
    "from decompression.fractal_interpolation import FractalInterpolationDecompressor\n",
    "from utils.metrics import CompressionMetrics\n",
    "from utils.plotting import CompressionVisualizer\n",
    "\n",
    "print(\"✅ All modules imported successfully!\")\n",
    "print(\"📊 Ready to demonstrate fractal compression methods\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Generation\n",
    "\n",
    "Let's start by generating different types of time series to test our compression methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_datasets(n_points=1000):\n",
    "    \"\"\"Generate various types of time series for testing.\"\"\"\n",
    "    datasets = {}\n",
    "    \n",
    "    # 1. Simple sine wave\n",
    "    t, y = TimeSeriesGenerator.sine_wave(n_points=n_points, frequency=2.0, amplitude=1.0, noise_level=0.05)\n",
    "    datasets['Simple Sine'] = (t, y)\n",
    "    \n",
    "    # 2. Multi-component signal\n",
    "    components = [\n",
    "        {'type': 'sine', 'frequency': 1.0, 'amplitude': 1.0},\n",
    "        {'type': 'sine', 'frequency': 3.0, 'amplitude': 0.5},\n",
    "        {'type': 'sine', 'frequency': 7.0, 'amplitude': 0.3},\n",
    "        {'type': 'sine', 'frequency': 15.0, 'amplitude': 0.1}\n",
    "    ]\n",
    "    t, y = TimeSeriesGenerator.multi_component_series(n_points=n_points, components=components)\n",
    "    datasets['Multi-Sine'] = (t, y)\n",
    "    \n",
    "    # 3. Fractal Brownian motion\n",
    "    t, y = TimeSeriesGenerator.fractal_brownian_motion(n_points=n_points, hurst=0.7, scale=1.0)\n",
    "    datasets['Fractal Brownian'] = (t, y)\n",
    "    \n",
    "    # 4. Stock price simulation\n",
    "    t, y = TimeSeriesGenerator.stock_price_simulation(n_points=n_points, volatility=0.2)\n",
    "    datasets['Stock Price'] = (t, y)\n",
    "    \n",
    "    # 5. Random walk\n",
    "    t, y = TimeSeriesGenerator.random_walk(n_points=n_points, step_size=0.1, drift=0.01)\n",
    "    datasets['Random Walk'] = (t, y)\n",
    "    \n",
    "    # Normalize all datasets\n",
    "    for name, (t, y) in datasets.items():\n",
    "        t_norm, y_norm = TimeSeriesLoader.preprocess_data(t, y, normalize=True)\n",
    "        datasets[name] = (t_norm, y_norm)\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "# Generate test datasets\n",
    "print(\"🔄 Generating test datasets...\")\n",
    "datasets = generate_test_datasets(n_points=800)\n",
    "print(f\"✅ Generated {len(datasets)} different time series types\")\n",
    "\n",
    "# Display dataset characteristics\n",
    "for name, (t, y) in datasets.items():\n",
    "    print(f\"   {name}: {len(y)} points, range [{np.min(y):.3f}, {np.max(y):.3f}], std={np.std(y):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize all generated datasets\n",
    "fig, axes = plt.subplots(3, 2, figsize=(15, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (name, (t, y)) in enumerate(datasets.items()):\n",
    "    if i < len(axes):\n",
    "        axes[i].plot(t, y, linewidth=1.5, alpha=0.8)\n",
    "        axes[i].set_title(f'{name} Time Series', fontsize=12, fontweight='bold')\n",
    "        axes[i].set_xlabel('Time')\n",
    "        axes[i].set_ylabel('Normalized Value')\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add statistics\n",
    "        mean_val = np.mean(y)\n",
    "        std_val = np.std(y)\n",
    "        axes[i].text(0.02, 0.95, f'μ={mean_val:.3f}\\nσ={std_val:.3f}', \n",
    "                    transform=axes[i].transAxes, verticalalignment='top',\n",
    "                    bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "# Remove empty subplot\n",
    "if len(datasets) < len(axes):\n",
    "    axes[-1].remove()\n",
    "\n",
    "plt.suptitle('Generated Time Series Datasets for Compression Testing', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Compression Method Setup\n",
    "\n",
    "Now let's set up our three compression methods with optimized parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize compression methods\n",
    "compressors = {\n",
    "    'IFS': IFSCompressor(\n",
    "        n_transformations=4,\n",
    "        max_iterations=100,\n",
    "        contractivity_bound=0.9\n",
    "    ),\n",
    "    'Fractal Coding': FractalCodingCompressor(\n",
    "        range_block_size=8,\n",
    "        domain_block_size=16,\n",
    "        overlap_factor=0.5\n",
    "    )\n",
    "}\n",
    "\n",
    "# Initialize fractal interpolation decompressor\n",
    "fif_decompressor = FractalInterpolationDecompressor()\n",
    "\n",
    "# Initialize metrics calculator and visualizer\n",
    "metrics_calc = CompressionMetrics()\n",
    "visualizer = CompressionVisualizer()\n",
    "\n",
    "print(\"🔧 Compression methods initialized:\")\n",
    "for name, compressor in compressors.items():\n",
    "    print(f\"   ✅ {name}: {compressor.__class__.__name__}\")\n",
    "print(f\"   ✅ Fractal Interpolation Decompressor: {fif_decompressor.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Single Dataset Detailed Analysis\n",
    "\n",
    "Let's start with a detailed analysis of one dataset to show the complete compression pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the multi-sine dataset for detailed analysis\n",
    "selected_dataset = 'Multi-Sine'\n",
    "time_data, value_data = datasets[selected_dataset]\n",
    "\n",
    "print(f\"🔍 Detailed Analysis: {selected_dataset}\")\n",
    "print(f\"   Data points: {len(value_data)}\")\n",
    "print(f\"   Original size: {len(value_data) * 8} bytes (assuming 8 bytes per float)\")\n",
    "print(f\"   Time range: [{time_data[0]:.3f}, {time_data[-1]:.3f}]\")\n",
    "print(f\"   Value range: [{np.min(value_data):.3f}, {np.max(value_data):.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_and_analyze(method_name, compressor, time_data, value_data, use_fif=False):\n",
    "    \"\"\"Compress data and analyze results.\"\"\"\n",
    "    print(f\"\\n🔄 Testing {method_name}...\")\n",
    "    \n",
    "    try:\n",
    "        # Compression\n",
    "        start_time = time.time()\n",
    "        comp_result = compressor.compress(time_data, value_data)\n",
    "        comp_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"   ✅ Compression completed in {comp_time:.3f}s\")\n",
    "        print(f\"   📊 Compression ratio: {comp_result.compression_ratio:.2f}x\")\n",
    "        print(f\"   💾 Space savings: {comp_result.space_savings:.1f}%\")\n",
    "        \n",
    "        # Decompression - try both methods\n",
    "        results = {}\n",
    "        \n",
    "        # Native decompression\n",
    "        start_time = time.time()\n",
    "        native_decomp = compressor.decompress(comp_result)\n",
    "        native_time = time.time() - start_time\n",
    "        \n",
    "        native_metrics = metrics_calc.comprehensive_evaluation(\n",
    "            value_data, native_decomp.reconstructed_data,\n",
    "            comp_result.original_size, comp_result.compressed_size,\n",
    "            comp_result.compression_time, native_decomp.decompression_time\n",
    "        )\n",
    "        \n",
    "        results['Native'] = {\n",
    "            'reconstructed': native_decomp.reconstructed_data,\n",
    "            'metrics': native_metrics,\n",
    "            'time': native_time\n",
    "        }\n",
    "        \n",
    "        print(f\"   🔄 Native decompression: {native_time:.3f}s, correlation={native_metrics['pearson_correlation']:.4f}\")\n",
    "        \n",
    "        # Fractal Interpolation decompression\n",
    "        if use_fif:\n",
    "            try:\n",
    "                start_time = time.time()\n",
    "                fif_decomp = fif_decompressor.decompress_with_fif(\n",
    "                    comp_result.compressed_data, \n",
    "                    len(value_data),\n",
    "                    (time_data[0], time_data[-1])\n",
    "                )\n",
    "                fif_time = time.time() - start_time\n",
    "                \n",
    "                fif_metrics = metrics_calc.comprehensive_evaluation(\n",
    "                    value_data, fif_decomp.reconstructed_data,\n",
    "                    comp_result.original_size, comp_result.compressed_size,\n",
    "                    comp_result.compression_time, fif_decomp.decompression_time\n",
    "                )\n",
    "                \n",
    "                results['FIF'] = {\n",
    "                    'reconstructed': fif_decomp.reconstructed_data,\n",
    "                    'metrics': fif_metrics,\n",
    "                    'time': fif_time\n",
    "                }\n",
    "                \n",
    "                print(f\"   🔄 FIF decompression: {fif_time:.3f}s, correlation={fif_metrics['pearson_correlation']:.4f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   ⚠️  FIF decompression failed: {e}\")\n",
    "        \n",
    "        return comp_result, results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ {method_name} failed: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Test all compression methods on the selected dataset\n",
    "compression_results = {}\n",
    "\n",
    "for method_name, compressor in compressors.items():\n",
    "    comp_result, decomp_results = compress_and_analyze(\n",
    "        method_name, compressor, time_data, value_data, use_fif=True\n",
    "    )\n",
    "    if comp_result is not None:\n",
    "        compression_results[method_name] = {\n",
    "            'compression': comp_result,\n",
    "            'decompression': decomp_results\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Detailed Before/After Comparison Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive before/after comparison\n",
    "def plot_detailed_comparison(time_data, original_data, compression_results, dataset_name):\n",
    "    \"\"\"Create detailed before/after comparison plots.\"\"\"\n",
    "    \n",
    "    n_methods = len(compression_results)\n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    \n",
    "    # Create grid layout\n",
    "    gs = fig.add_gridspec(4, 3, hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    # Original signal (top row, spanning all columns)\n",
    "    ax_orig = fig.add_subplot(gs[0, :])\n",
    "    ax_orig.plot(time_data, original_data, 'b-', linewidth=2, label='Original Signal')\n",
    "    ax_orig.set_title(f'Original {dataset_name} Time Series', fontsize=16, fontweight='bold')\n",
    "    ax_orig.set_ylabel('Normalized Value', fontsize=12)\n",
    "    ax_orig.grid(True, alpha=0.3)\n",
    "    ax_orig.legend(fontsize=12)\n",
    "    \n",
    "    # Add original signal statistics\n",
    "    orig_stats = f'Length: {len(original_data)}\\nMean: {np.mean(original_data):.4f}\\nStd: {np.std(original_data):.4f}'\n",
    "    ax_orig.text(0.02, 0.95, orig_stats, transform=ax_orig.transAxes, \n",
    "                verticalalignment='top', fontsize=10,\n",
    "                bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
    "    \n",
    "    # Compression results (remaining rows)\n",
    "    method_colors = ['red', 'green', 'orange', 'purple']\n",
    "    \n",
    "    row = 1\n",
    "    for method_idx, (method_name, results) in enumerate(compression_results.items()):\n",
    "        color = method_colors[method_idx % len(method_colors)]\n",
    "        \n",
    "        # Reconstruction comparison\n",
    "        ax_recon = fig.add_subplot(gs[row, 0])\n",
    "        ax_recon.plot(time_data, original_data, 'b-', alpha=0.7, linewidth=1.5, label='Original')\n",
    "        \n",
    "        # Plot native reconstruction\n",
    "        if 'Native' in results['decompression']:\n",
    "            native_recon = results['decompression']['Native']['reconstructed']\n",
    "            ax_recon.plot(time_data, native_recon, '--', color=color, \n",
    "                         linewidth=2, alpha=0.8, label=f'{method_name} (Native)')\n",
    "        \n",
    "        # Plot FIF reconstruction if available\n",
    "        if 'FIF' in results['decompression']:\n",
    "            fif_recon = results['decompression']['FIF']['reconstructed']\n",
    "            ax_recon.plot(time_data, fif_recon, ':', color=color, \n",
    "                         linewidth=2, alpha=0.8, label=f'{method_name} (FIF)')\n",
    "        \n",
    "        ax_recon.set_title(f'{method_name} Reconstruction', fontsize=12, fontweight='bold')\n",
    "        ax_recon.set_ylabel('Value', fontsize=10)\n",
    "        ax_recon.grid(True, alpha=0.3)\n",
    "        ax_recon.legend(fontsize=9)\n",
    "        \n",
    "        # Error plot\n",
    "        ax_error = fig.add_subplot(gs[row, 1])\n",
    "        \n",
    "        if 'Native' in results['decompression']:\n",
    "            native_recon = results['decompression']['Native']['reconstructed']\n",
    "            error = original_data - native_recon\n",
    "            ax_error.plot(time_data, error, '-', color=color, linewidth=1.5, alpha=0.8, label='Native Error')\n",
    "        \n",
    "        if 'FIF' in results['decompression']:\n",
    "            fif_recon = results['decompression']['FIF']['reconstructed']\n",
    "            fif_error = original_data - fif_recon\n",
    "            ax_error.plot(time_data, fif_error, '--', color=color, linewidth=1.5, alpha=0.8, label='FIF Error')\n",
    "        \n",
    "        ax_error.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "        ax_error.set_title(f'{method_name} Reconstruction Error', fontsize=12, fontweight='bold')\n",
    "        ax_error.set_ylabel('Error', fontsize=10)\n",
    "        ax_error.grid(True, alpha=0.3)\n",
    "        ax_error.legend(fontsize=9)\n",
    "        \n",
    "        # Metrics summary\n",
    "        ax_metrics = fig.add_subplot(gs[row, 2])\n",
    "        ax_metrics.axis('off')\n",
    "        \n",
    "        # Prepare metrics text\n",
    "        comp_metrics = results['compression']\n",
    "        \n",
    "        metrics_text = f\"COMPRESSION METRICS\\n\\n\"\n",
    "        metrics_text += f\"Ratio: {comp_metrics.compression_ratio:.2f}x\\n\"\n",
    "        metrics_text += f\"Space Savings: {comp_metrics.space_savings:.1f}%\\n\"\n",
    "        metrics_text += f\"Comp. Time: {comp_metrics.compression_time:.3f}s\\n\\n\"\n",
    "        \n",
    "        if 'Native' in results['decompression']:\n",
    "            native_metrics = results['decompression']['Native']['metrics']\n",
    "            metrics_text += f\"NATIVE DECOMPRESSION\\n\"\n",
    "            metrics_text += f\"RMSE: {native_metrics['rmse']:.4f}\\n\"\n",
    "            metrics_text += f\"Correlation: {native_metrics['pearson_correlation']:.4f}\\n\"\n",
    "            metrics_text += f\"SNR: {native_metrics['snr_db']:.1f} dB\\n\"\n",
    "            metrics_text += f\"SSIM: {native_metrics['ssim']:.4f}\\n\"\n",
    "            metrics_text += f\"Time: {results['decompression']['Native']['time']:.3f}s\\n\\n\"\n",
    "        \n",
    "        if 'FIF' in results['decompression']:\n",
    "            fif_metrics = results['decompression']['FIF']['metrics']\n",
    "            metrics_text += f\"FIF DECOMPRESSION\\n\"\n",
    "            metrics_text += f\"RMSE: {fif_metrics['rmse']:.4f}\\n\"\n",
    "            metrics_text += f\"Correlation: {fif_metrics['pearson_correlation']:.4f}\\n\"\n",
    "            metrics_text += f\"SNR: {fif_metrics['snr_db']:.1f} dB\\n\"\n",
    "            metrics_text += f\"SSIM: {fif_metrics['ssim']:.4f}\\n\"\n",
    "            metrics_text += f\"Time: {results['decompression']['FIF']['time']:.3f}s\"\n",
    "        \n",
    "        ax_metrics.text(0.05, 0.95, metrics_text, transform=ax_metrics.transAxes,\n",
    "                       verticalalignment='top', fontfamily='monospace', fontsize=9,\n",
    "                       bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))\n",
    "        \n",
    "        row += 1\n",
    "    \n",
    "    plt.suptitle(f'Detailed Compression Analysis: {dataset_name}', fontsize=18, fontweight='bold')\n",
    "    return fig\n",
    "\n",
    "# Create the detailed comparison plot\n",
    "if compression_results:\n",
    "    fig = plot_detailed_comparison(time_data, value_data, compression_results, selected_dataset)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"❌ No compression results to display\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Frequency Domain Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency domain analysis for the selected dataset\n",
    "def plot_frequency_analysis(time_data, original_data, compression_results):\n",
    "    \"\"\"Plot frequency domain analysis.\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "    \n",
    "    # Compute FFT of original signal\n",
    "    fft_original = np.fft.fft(original_data)\n",
    "    freqs = np.fft.fftfreq(len(original_data), d=time_data[1]-time_data[0])\n",
    "    \n",
    "    # Only plot positive frequencies\n",
    "    pos_mask = freqs > 0\n",
    "    freqs_pos = freqs[pos_mask]\n",
    "    fft_orig_pos = fft_original[pos_mask]\n",
    "    \n",
    "    # Plot 1: Original magnitude spectrum\n",
    "    axes[0,0].loglog(freqs_pos, np.abs(fft_orig_pos), 'b-', linewidth=2, label='Original')\n",
    "    axes[0,0].set_title('Magnitude Spectrum Comparison', fontsize=12, fontweight='bold')\n",
    "    axes[0,0].set_xlabel('Frequency')\n",
    "    axes[0,0].set_ylabel('Magnitude')\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Phase spectrum\n",
    "    axes[0,1].plot(freqs_pos, np.angle(fft_orig_pos), 'b-', linewidth=2, alpha=0.7, label='Original')\n",
    "    axes[0,1].set_title('Phase Spectrum Comparison', fontsize=12, fontweight='bold')\n",
    "    axes[0,1].set_xlabel('Frequency')\n",
    "    axes[0,1].set_ylabel('Phase (radians)')\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot reconstructed signals' frequency content\n",
    "    colors = ['red', 'green', 'orange', 'purple']\n",
    "    \n",
    "    for method_idx, (method_name, results) in enumerate(compression_results.items()):\n",
    "        color = colors[method_idx % len(colors)]\n",
    "        \n",
    "        if 'Native' in results['decompression']:\n",
    "            recon_data = results['decompression']['Native']['reconstructed']\n",
    "            fft_recon = np.fft.fft(recon_data)\n",
    "            fft_recon_pos = fft_recon[pos_mask]\n",
    "            \n",
    "            # Magnitude spectrum\n",
    "            axes[0,0].loglog(freqs_pos, np.abs(fft_recon_pos), '--', \n",
    "                           color=color, linewidth=2, alpha=0.8, label=f'{method_name}')\n",
    "            \n",
    "            # Phase spectrum\n",
    "            axes[0,1].plot(freqs_pos, np.angle(fft_recon_pos), '--', \n",
    "                          color=color, linewidth=2, alpha=0.8, label=f'{method_name}')\n",
    "    \n",
    "    axes[0,0].legend()\n",
    "    axes[0,1].legend()\n",
    "    \n",
    "    # Plot 3: Magnitude error\n",
    "    for method_idx, (method_name, results) in enumerate(compression_results.items()):\n",
    "        color = colors[method_idx % len(colors)]\n",
    "        \n",
    "        if 'Native' in results['decompression']:\n",
    "            recon_data = results['decompression']['Native']['reconstructed']\n",
    "            fft_recon = np.fft.fft(recon_data)\n",
    "            fft_recon_pos = fft_recon[pos_mask]\n",
    "            \n",
    "            mag_error = np.abs(np.abs(fft_orig_pos) - np.abs(fft_recon_pos))\n",
    "            axes[1,0].semilogy(freqs_pos, mag_error, '-', color=color, \n",
    "                             linewidth=2, alpha=0.8, label=f'{method_name}')\n",
    "    \n",
    "    axes[1,0].set_title('Magnitude Reconstruction Error', fontsize=12, fontweight='bold')\n",
    "    axes[1,0].set_xlabel('Frequency')\n",
    "    axes[1,0].set_ylabel('Magnitude Error')\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    axes[1,0].legend()\n",
    "    \n",
    "    # Plot 4: Frequency domain metrics\n",
    "    axes[1,1].axis('off')\n",
    "    \n",
    "    freq_metrics_text = \"FREQUENCY DOMAIN METRICS\\n\\n\"\n",
    "    \n",
    "    for method_name, results in compression_results.items():\n",
    "        if 'Native' in results['decompression']:\n",
    "            recon_data = results['decompression']['Native']['reconstructed']\n",
    "            freq_metrics = metrics_calc.frequency_domain_error(original_data, recon_data)\n",
    "            \n",
    "            freq_metrics_text += f\"{method_name}:\\n\"\n",
    "            freq_metrics_text += f\"  Mag Error: {freq_metrics['magnitude_error']:.4f}\\n\"\n",
    "            freq_metrics_text += f\"  Phase Error: {freq_metrics['phase_error']:.4f}\\n\"\n",
    "            freq_metrics_text += f\"  Spectral Corr: {freq_metrics['spectral_correlation']:.4f}\\n\\n\"\n",
    "    \n",
    "    axes[1,1].text(0.05, 0.95, freq_metrics_text, transform=axes[1,1].transAxes,\n",
    "                  verticalalignment='top', fontfamily='monospace', fontsize=10,\n",
    "                  bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8))\n",
    "    \n",
    "    plt.suptitle(f'Frequency Domain Analysis: {selected_dataset}', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "if compression_results:\n",
    "    freq_fig = plot_frequency_analysis(time_data, value_data, compression_results)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comprehensive Dataset Comparison\n",
    "\n",
    "Now let's test all compression methods on all datasets and create a comprehensive comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test all methods on all datasets\n",
    "def test_all_combinations():\n",
    "    \"\"\"Test all compression methods on all datasets.\"\"\"\n",
    "    \n",
    "    all_results = {}\n",
    "    \n",
    "    for dataset_name, (t, y) in datasets.items():\n",
    "        print(f\"\\n🔄 Testing dataset: {dataset_name}\")\n",
    "        dataset_results = {}\n",
    "        \n",
    "        for method_name, compressor in compressors.items():\n",
    "            try:\n",
    "                print(f\"   Testing {method_name}...\", end=\" \")\n",
    "                \n",
    "                # Compress\n",
    "                comp_result = compressor.compress(t, y)\n",
    "                \n",
    "                # Decompress (native method)\n",
    "                decomp_result = compressor.decompress(comp_result)\n",
    "                \n",
    "                # Calculate metrics\n",
    "                metrics = metrics_calc.comprehensive_evaluation(\n",
    "                    y, decomp_result.reconstructed_data,\n",
    "                    comp_result.original_size, comp_result.compressed_size,\n",
    "                    comp_result.compression_time, decomp_result.decompression_time\n",
    "                )\n",
    "                \n",
    "                dataset_results[method_name] = {\n",
    "                    'metrics': metrics,\n",
    "                    'reconstructed': decomp_result.reconstructed_data,\n",
    "                    'original': y\n",
    "                }\n",
    "                \n",
    "                print(f\"✅ Ratio: {metrics['compression_ratio']:.2f}x, Corr: {metrics['pearson_correlation']:.3f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"❌ Failed: {str(e)[:50]}...\")\n",
    "                dataset_results[method_name] = {'error': str(e)}\n",
    "        \n",
    "        all_results[dataset_name] = dataset_results\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "# Run comprehensive testing\n",
    "print(\"🚀 Starting comprehensive testing of all methods on all datasets...\")\n",
    "all_results = test_all_combinations()\n",
    "print(\"\\n✅ Comprehensive testing completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary comparison table\n",
    "def create_results_dataframe(all_results):\n",
    "    \"\"\"Create a pandas DataFrame summarizing all results.\"\"\"\n",
    "    \n",
    "    rows = []\n",
    "    \n",
    "    for dataset_name, dataset_results in all_results.items():\n",
    "        for method_name, method_results in dataset_results.items():\n",
    "            if 'error' not in method_results:\n",
    "                metrics = method_results['metrics']\n",
    "                row = {\n",
    "                    'Dataset': dataset_name,\n",
    "                    'Method': method_name,\n",
    "                    'Compression Ratio': metrics['compression_ratio'],\n",
    "                    'Space Savings (%)': metrics['space_savings_percent'],\n",
    "                    'RMSE': metrics['rmse'],\n",
    "                    'Correlation': metrics['pearson_correlation'],\n",
    "                    'SNR (dB)': metrics['snr_db'],\n",
    "                    'PSNR (dB)': metrics['psnr_db'],\n",
    "                    'SSIM': metrics['ssim'],\n",
    "                    'Total Time (s)': metrics['total_time'],\n",
    "                    'Fractal Dim Error': metrics.get('fractal_fractal_dimension_error', 0),\n",
    "                    'Hurst Error': metrics.get('fractal_hurst_exponent_error', 0)\n",
    "                }\n",
    "                rows.append(row)\n",
    "    \n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = create_results_dataframe(all_results)\n",
    "\n",
    "# Display summary table\n",
    "print(\"📊 COMPREHENSIVE RESULTS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Show key metrics\n",
    "display_columns = ['Dataset', 'Method', 'Compression Ratio', 'Correlation', 'RMSE', 'Total Time (s)']\n",
    "summary_df = results_df[display_columns].round(4)\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# Show best performers\n",
    "print(\"\\n🏆 BEST PERFORMERS:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for metric in ['Compression Ratio', 'Correlation', 'RMSE']:\n",
    "    if metric == 'RMSE':\n",
    "        best_row = results_df.loc[results_df[metric].idxmin()]\n",
    "        print(f\"Lowest {metric}: {best_row['Method']} on {best_row['Dataset']} ({best_row[metric]:.4f})\")\n",
    "    else:\n",
    "        best_row = results_df.loc[results_df[metric].idxmax()]\n",
    "        print(f\"Best {metric}: {best_row['Method']} on {best_row['Dataset']} ({best_row[metric]:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization of all results\n",
    "def plot_comprehensive_comparison(results_df):\n",
    "    \"\"\"Create comprehensive comparison plots.\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "    \n",
    "    # Plot 1: Compression Ratio by Dataset and Method\n",
    "    pivot_ratio = results_df.pivot(index='Dataset', columns='Method', values='Compression Ratio')\n",
    "    sns.heatmap(pivot_ratio, annot=True, fmt='.2f', cmap='YlOrRd', ax=axes[0,0])\n",
    "    axes[0,0].set_title('Compression Ratio by Dataset and Method', fontweight='bold')\n",
    "    axes[0,0].set_ylabel('')\n",
    "    \n",
    "    # Plot 2: Correlation by Dataset and Method\n",
    "    pivot_corr = results_df.pivot(index='Dataset', columns='Method', values='Correlation')\n",
    "    sns.heatmap(pivot_corr, annot=True, fmt='.3f', cmap='RdYlGn', ax=axes[0,1], vmin=0, vmax=1)\n",
    "    axes[0,1].set_title('Reconstruction Quality (Correlation)', fontweight='bold')\n",
    "    axes[0,1].set_ylabel('')\n",
    "    \n",
    "    # Plot 3: Processing Time\n",
    "    pivot_time = results_df.pivot(index='Dataset', columns='Method', values='Total Time (s)')\n",
    "    sns.heatmap(pivot_time, annot=True, fmt='.3f', cmap='YlOrRd', ax=axes[0,2])\n",
    "    axes[0,2].set_title('Total Processing Time (s)', fontweight='bold')\n",
    "    axes[0,2].set_ylabel('')\n",
    "    \n",
    "    # Plot 4: Trade-off scatter plot\n",
    "    colors = {'IFS': 'red', 'Fractal Coding': 'green'}\n",
    "    for method in results_df['Method'].unique():\n",
    "        method_data = results_df[results_df['Method'] == method]\n",
    "        axes[1,0].scatter(method_data['Compression Ratio'], method_data['Correlation'], \n",
    "                         label=method, alpha=0.7, s=100, c=colors.get(method, 'blue'))\n",
    "    \n",
    "    axes[1,0].set_xlabel('Compression Ratio')\n",
    "    axes[1,0].set_ylabel('Reconstruction Quality (Correlation)')\n",
    "    axes[1,0].set_title('Quality vs Compression Trade-off', fontweight='bold')\n",
    "    axes[1,0].legend()\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 5: Method comparison radar chart data preparation\n",
    "    method_avg = results_df.groupby('Method').agg({\n",
    "        'Compression Ratio': 'mean',\n",
    "        'Correlation': 'mean',\n",
    "        'SSIM': 'mean',\n",
    "        'SNR (dB)': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Normalize metrics for radar chart (0-1 scale)\n",
    "    metrics_to_plot = ['Compression Ratio', 'Correlation', 'SSIM', 'SNR (dB)']\n",
    "    \n",
    "    # Bar chart instead of radar for simplicity\n",
    "    x_pos = np.arange(len(method_avg))\n",
    "    width = 0.2\n",
    "    \n",
    "    for i, metric in enumerate(metrics_to_plot[:3]):  # Show first 3 metrics\n",
    "        if metric == 'SNR (dB)':\n",
    "            # Normalize SNR to 0-1 scale\n",
    "            values = (method_avg[metric] - method_avg[metric].min()) / (method_avg[metric].max() - method_avg[metric].min())\n",
    "        elif metric == 'Compression Ratio':\n",
    "            # Normalize compression ratio\n",
    "            values = method_avg[metric] / method_avg[metric].max()\n",
    "        else:\n",
    "            values = method_avg[metric]\n",
    "        \n",
    "        axes[1,1].bar(x_pos + i*width, values, width, label=metric, alpha=0.8)\n",
    "    \n",
    "    axes[1,1].set_xlabel('Method')\n",
    "    axes[1,1].set_ylabel('Normalized Score')\n",
    "    axes[1,1].set_title('Average Performance Comparison', fontweight='bold')\n",
    "    axes[1,1].set_xticks(x_pos + width)\n",
    "    axes[1,1].set_xticklabels(method_avg['Method'])\n",
    "    axes[1,1].legend()\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 6: Dataset difficulty ranking\n",
    "    dataset_avg = results_df.groupby('Dataset').agg({\n",
    "        'Correlation': 'mean',\n",
    "        'Compression Ratio': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    dataset_avg = dataset_avg.sort_values('Correlation', ascending=False)\n",
    "    \n",
    "    bars1 = axes[1,2].bar(range(len(dataset_avg)), dataset_avg['Correlation'], \n",
    "                         alpha=0.7, label='Avg Correlation', color='skyblue')\n",
    "    axes[1,2].set_ylabel('Average Correlation', color='blue')\n",
    "    axes[1,2].tick_params(axis='y', labelcolor='blue')\n",
    "    \n",
    "    # Secondary y-axis for compression ratio\n",
    "    ax2 = axes[1,2].twinx()\n",
    "    bars2 = ax2.bar(range(len(dataset_avg)), dataset_avg['Compression Ratio'], \n",
    "                   alpha=0.7, label='Avg Compression Ratio', color='orange', width=0.6)\n",
    "    ax2.set_ylabel('Average Compression Ratio', color='orange')\n",
    "    ax2.tick_params(axis='y', labelcolor='orange')\n",
    "    \n",
    "    axes[1,2].set_xlabel('Dataset')\n",
    "    axes[1,2].set_title('Dataset Compression Difficulty', fontweight='bold')\n",
    "    axes[1,2].set_xticks(range(len(dataset_avg)))\n",
    "    axes[1,2].set_xticklabels(dataset_avg['Dataset'], rotation=45, ha='right')\n",
    "    \n",
    "    plt.suptitle('Comprehensive Fractal Compression Analysis', fontsize=18, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create comprehensive comparison plot\n",
    "if not results_df.empty:\n",
    "    comp_fig = plot_comprehensive_comparison(results_df)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"❌ No results to plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Fractal Properties Analysis\n",
    "\n",
    "Let's examine how well our compression methods preserve fractal properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze fractal properties preservation\n",
    "def analyze_fractal_properties(all_results):\n",
    "    \"\"\"Analyze how well fractal properties are preserved.\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "    \n",
    "    fractal_data = []\n",
    "    \n",
    "    for dataset_name, dataset_results in all_results.items():\n",
    "        for method_name, method_results in dataset_results.items():\n",
    "            if 'error' not in method_results and 'metrics' in method_results:\n",
    "                metrics = method_results['metrics']\n",
    "                \n",
    "                # Extract fractal metrics\n",
    "                fractal_info = {\n",
    "                    'Dataset': dataset_name,\n",
    "                    'Method': method_name,\n",
    "                    'Original FD': metrics.get('fractal_fractal_dimension_original', 1.0),\n",
    "                    'Reconstructed FD': metrics.get('fractal_fractal_dimension_reconstructed', 1.0),\n",
    "                    'FD Error': metrics.get('fractal_fractal_dimension_error', 0.0),\n",
    "                    'Original Hurst': metrics.get('fractal_hurst_exponent_original', 0.5),\n",
    "                    'Reconstructed Hurst': metrics.get('fractal_hurst_exponent_reconstructed', 0.5),\n",
    "                    'Hurst Error': metrics.get('fractal_hurst_exponent_error', 0.0)\n",
    "                }\n",
    "                fractal_data.append(fractal_info)\n",
    "    \n",
    "    if fractal_data:\n",
    "        fractal_df = pd.DataFrame(fractal_data)\n",
    "        \n",
    "        # Plot 1: Fractal Dimension preservation\n",
    "        for method in fractal_df['Method'].unique():\n",
    "            method_data = fractal_df[fractal_df['Method'] == method]\n",
    "            axes[0,0].scatter(method_data['Original FD'], method_data['Reconstructed FD'], \n",
    "                            label=method, alpha=0.7, s=100)\n",
    "        \n",
    "        # Perfect correlation line\n",
    "        min_fd = min(fractal_df['Original FD'].min(), fractal_df['Reconstructed FD'].min())\n",
    "        max_fd = max(fractal_df['Original FD'].max(), fractal_df['Reconstructed FD'].max())\n",
    "        axes[0,0].plot([min_fd, max_fd], [min_fd, max_fd], 'r--', alpha=0.5, label='Perfect Preservation')\n",
    "        \n",
    "        axes[0,0].set_xlabel('Original Fractal Dimension')\n",
    "        axes[0,0].set_ylabel('Reconstructed Fractal Dimension')\n",
    "        axes[0,0].set_title('Fractal Dimension Preservation', fontweight='bold')\n",
    "        axes[0,0].legend()\n",
    "        axes[0,0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 2: Hurst Exponent preservation\n",
    "        for method in fractal_df['Method'].unique():\n",
    "            method_data = fractal_df[fractal_df['Method'] == method]\n",
    "            axes[0,1].scatter(method_data['Original Hurst'], method_data['Reconstructed Hurst'], \n",
    "                            label=method, alpha=0.7, s=100)\n",
    "        \n",
    "        # Perfect correlation line\n",
    "        axes[0,1].plot([0, 1], [0, 1], 'r--', alpha=0.5, label='Perfect Preservation')\n",
    "        \n",
    "        axes[0,1].set_xlabel('Original Hurst Exponent')\n",
    "        axes[0,1].set_ylabel('Reconstructed Hurst Exponent')\n",
    "        axes[0,1].set_title('Hurst Exponent Preservation', fontweight='bold')\n",
    "        axes[0,1].legend()\n",
    "        axes[0,1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 3: Error distributions\n",
    "        methods = fractal_df['Method'].unique()\n",
    "        x_pos = np.arange(len(methods))\n",
    "        \n",
    "        fd_errors = [fractal_df[fractal_df['Method'] == method]['FD Error'].mean() for method in methods]\n",
    "        hurst_errors = [fractal_df[fractal_df['Method'] == method]['Hurst Error'].mean() for method in methods]\n",
    "        \n",
    "        width = 0.35\n",
    "        axes[1,0].bar(x_pos - width/2, fd_errors, width, label='Fractal Dimension Error', alpha=0.8)\n",
    "        axes[1,0].bar(x_pos + width/2, hurst_errors, width, label='Hurst Exponent Error', alpha=0.8)\n",
    "        \n",
    "        axes[1,0].set_xlabel('Method')\n",
    "        axes[1,0].set_ylabel('Average Error')\n",
    "        axes[1,0].set_title('Average Fractal Property Errors', fontweight='bold')\n",
    "        axes[1,0].set_xticks(x_pos)\n",
    "        axes[1,0].set_xticklabels(methods)\n",
    "        axes[1,0].legend()\n",
    "        axes[1,0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 4: Dataset-wise fractal preservation\n",
    "        dataset_fractal = fractal_df.groupby('Dataset').agg({\n",
    "            'FD Error': 'mean',\n",
    "            'Hurst Error': 'mean'\n",
    "        }).reset_index()\n",
    "        \n",
    "        x_pos = np.arange(len(dataset_fractal))\n",
    "        axes[1,1].bar(x_pos - width/2, dataset_fractal['FD Error'], width, \n",
    "                     label='Avg FD Error', alpha=0.8)\n",
    "        axes[1,1].bar(x_pos + width/2, dataset_fractal['Hurst Error'], width, \n",
    "                     label='Avg Hurst Error', alpha=0.8)\n",
    "        \n",
    "        axes[1,1].set_xlabel('Dataset')\n",
    "        axes[1,1].set_ylabel('Average Error')\n",
    "        axes[1,1].set_title('Fractal Property Preservation by Dataset', fontweight='bold')\n",
    "        axes[1,1].set_xticks(x_pos)\n",
    "        axes[1,1].set_xticklabels(dataset_fractal['Dataset'], rotation=45, ha='right')\n",
    "        axes[1,1].legend()\n",
    "        axes[1,1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.suptitle('Fractal Properties Preservation Analysis', fontsize=16, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        return fig, fractal_df\n",
    "    else:\n",
    "        print(\"❌ No fractal data available for analysis\")\n",
    "        return None, None\n",
    "\n",
    "# Analyze fractal properties\n",
    "fractal_fig, fractal_df = analyze_fractal_properties(all_results)\n",
    "if fractal_fig:\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n📊 FRACTAL PROPERTIES SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "    if fractal_df is not None and not fractal_df.empty:\n",
    "        summary_fractal = fractal_df.groupby('Method').agg({\n",
    "            'FD Error': ['mean', 'std'],\n",
    "            'Hurst Error': ['mean', 'std']\n",
    "        }).round(4)\n",
    "        print(summary_fractal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Interactive Parameter Exploration\n",
    "\n",
    "Let's explore how different parameters affect compression performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter sensitivity analysis\n",
    "def parameter_sensitivity_analysis():\n",
    "    \"\"\"Analyze sensitivity to different parameters.\"\"\"\n",
    "    \n",
    "    # Use the multi-sine dataset for analysis\n",
    "    time_data, value_data = datasets['Multi-Sine']\n",
    "    \n",
    "    # IFS parameter sensitivity\n",
    "    print(\"🔄 IFS Parameter Sensitivity Analysis...\")\n",
    "    ifs_results = {}\n",
    "    \n",
    "    n_transform_values = [2, 3, 4, 5, 6]\n",
    "    for n_transforms in n_transform_values:\n",
    "        try:\n",
    "            compressor = IFSCompressor(n_transformations=n_transforms, max_iterations=50)\n",
    "            comp_result = compressor.compress(time_data, value_data)\n",
    "            decomp_result = compressor.decompress(comp_result)\n",
    "            \n",
    "            correlation = metrics_calc.pearson_correlation(value_data, decomp_result.reconstructed_data)\n",
    "            \n",
    "            ifs_results[n_transforms] = {\n",
    "                'compression_ratio': comp_result.compression_ratio,\n",
    "                'correlation': correlation,\n",
    "                'time': comp_result.compression_time + decomp_result.decompression_time\n",
    "            }\n",
    "            \n",
    "            print(f\"   {n_transforms} transforms: ratio={comp_result.compression_ratio:.2f}, corr={correlation:.3f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   {n_transforms} transforms: Failed ({str(e)[:30]}...)\")\n",
    "    \n",
    "    # Fractal Coding parameter sensitivity\n",
    "    print(\"\\n🔄 Fractal Coding Parameter Sensitivity Analysis...\")\n",
    "    fc_results = {}\n",
    "    \n",
    "    block_sizes = [4, 6, 8, 10, 12]\n",
    "    for block_size in block_sizes:\n",
    "        try:\n",
    "            compressor = FractalCodingCompressor(\n",
    "                range_block_size=block_size, \n",
    "                domain_block_size=block_size*2\n",
    "            )\n",
    "            comp_result = compressor.compress(time_data, value_data)\n",
    "            decomp_result = compressor.decompress(comp_result)\n",
    "            \n",
    "            correlation = metrics_calc.pearson_correlation(value_data, decomp_result.reconstructed_data)\n",
    "            \n",
    "            fc_results[block_size] = {\n",
    "                'compression_ratio': comp_result.compression_ratio,\n",
    "                'correlation': correlation,\n",
    "                'time': comp_result.compression_time + decomp_result.decompression_time\n",
    "            }\n",
    "            \n",
    "            print(f\"   Block size {block_size}: ratio={comp_result.compression_ratio:.2f}, corr={correlation:.3f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   Block size {block_size}: Failed ({str(e)[:30]}...)\")\n",
    "    \n",
    "    return ifs_results, fc_results\n",
    "\n",
    "# Run parameter sensitivity analysis\n",
    "ifs_param_results, fc_param_results = parameter_sensitivity_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot parameter sensitivity results\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# IFS parameter plots\n",
    "if ifs_param_results:\n",
    "    params = list(ifs_param_results.keys())\n",
    "    ratios = [ifs_param_results[p]['compression_ratio'] for p in params]\n",
    "    correlations = [ifs_param_results[p]['correlation'] for p in params]\n",
    "    times = [ifs_param_results[p]['time'] for p in params]\n",
    "    \n",
    "    axes[0,0].plot(params, ratios, 'bo-', linewidth=2, markersize=8)\n",
    "    axes[0,0].set_xlabel('Number of Transformations')\n",
    "    axes[0,0].set_ylabel('Compression Ratio')\n",
    "    axes[0,0].set_title('IFS: Compression Ratio vs Parameters', fontweight='bold')\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[0,1].plot(params, correlations, 'ro-', linewidth=2, markersize=8)\n",
    "    axes[0,1].set_xlabel('Number of Transformations')\n",
    "    axes[0,1].set_ylabel('Correlation')\n",
    "    axes[0,1].set_title('IFS: Quality vs Parameters', fontweight='bold')\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[0,2].plot(params, times, 'go-', linewidth=2, markersize=8)\n",
    "    axes[0,2].set_xlabel('Number of Transformations')\n",
    "    axes[0,2].set_ylabel('Total Time (s)')\n",
    "    axes[0,2].set_title('IFS: Processing Time vs Parameters', fontweight='bold')\n",
    "    axes[0,2].grid(True, alpha=0.3)\n",
    "\n",
    "# Fractal Coding parameter plots\n",
    "if fc_param_results:\n",
    "    params = list(fc_param_results.keys())\n",
    "    ratios = [fc_param_results[p]['compression_ratio'] for p in params]\n",
    "    correlations = [fc_param_results[p]['correlation'] for p in params]\n",
    "    times = [fc_param_results[p]['time'] for p in params]\n",
    "    \n",
    "    axes[1,0].plot(params, ratios, 'bo-', linewidth=2, markersize=8)\n",
    "    axes[1,0].set_xlabel('Range Block Size')\n",
    "    axes[1,0].set_ylabel('Compression Ratio')\n",
    "    axes[1,0].set_title('Fractal Coding: Compression Ratio vs Block Size', fontweight='bold')\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1,1].plot(params, correlations, 'ro-', linewidth=2, markersize=8)\n",
    "    axes[1,1].set_xlabel('Range Block Size')\n",
    "    axes[1,1].set_ylabel('Correlation')\n",
    "    axes[1,1].set_title('Fractal Coding: Quality vs Block Size', fontweight='bold')\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1,2].plot(params, times, 'go-', linewidth=2, markersize=8)\n",
    "    axes[1,2].set_xlabel('Range Block Size')\n",
    "    axes[1,2].set_ylabel('Total Time (s)')\n",
    "    axes[1,2].set_title('Fractal Coding: Processing Time vs Block Size', fontweight='bold')\n",
    "    axes[1,2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Parameter Sensitivity Analysis', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusions and Summary\n",
    "\n",
    "Let's summarize our findings and provide recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final summary and recommendations\n",
    "def generate_final_summary(results_df, fractal_df):\n",
    "    \"\"\"Generate comprehensive summary and recommendations.\"\"\"\n",
    "    \n",
    "    print(\"🎯 FRACTAL TIME SERIES COMPRESSION - FINAL SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    if not results_df.empty:\n",
    "        # Overall performance\n",
    "        print(\"\\n📊 OVERALL PERFORMANCE:\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        overall_stats = results_df.groupby('Method').agg({\n",
    "            'Compression Ratio': ['mean', 'std', 'min', 'max'],\n",
    "            'Correlation': ['mean', 'std', 'min', 'max'],\n",
    "            'Total Time (s)': ['mean', 'std']\n",
    "        }).round(3)\n",
    "        \n",
    "        for method in results_df['Method'].unique():\n",
    "            method_data = results_df[results_df['Method'] == method]\n",
    "            \n",
    "            print(f\"\\n{method}:\")\n",
    "            print(f\"  Compression Ratio: {method_data['Compression Ratio'].mean():.2f} ± {method_data['Compression Ratio'].std():.2f}\")\n",
    "            print(f\"  Correlation: {method_data['Correlation'].mean():.3f} ± {method_data['Correlation'].std():.3f}\")\n",
    "            print(f\"  Processing Time: {method_data['Total Time (s)'].mean():.3f} ± {method_data['Total Time (s)'].std():.3f} seconds\")\n",
    "            print(f\"  Best Dataset: {method_data.loc[method_data['Correlation'].idxmax(), 'Dataset']}\")\n",
    "            print(f\"  Worst Dataset: {method_data.loc[method_data['Correlation'].idxmin(), 'Dataset']}\")\n",
    "        \n",
    "        # Method comparison\n",
    "        print(\"\\n🏆 METHOD COMPARISON:\")\n",
    "        print(\"-\" * 25)\n",
    "        \n",
    "        best_compression = results_df.loc[results_df['Compression Ratio'].idxmax()]\n",
    "        best_quality = results_df.loc[results_df['Correlation'].idxmax()]\n",
    "        fastest = results_df.loc[results_df['Total Time (s)'].idxmin()]\n",
    "        \n",
    "        print(f\"Best Compression: {best_compression['Method']} on {best_compression['Dataset']} ({best_compression['Compression Ratio']:.2f}x)\")\n",
    "        print(f\"Best Quality: {best_quality['Method']} on {best_quality['Dataset']} (r={best_quality['Correlation']:.4f})\")\n",
    "        print(f\"Fastest: {fastest['Method']} on {fastest['Dataset']} ({fastest['Total Time (s)']:.3f}s)\")\n",
    "        \n",
    "        # Dataset analysis\n",
    "        print(\"\\n📈 DATASET ANALYSIS:\")\n",
    "        print(\"-\" * 20)\n",
    "        \n",
    "        dataset_difficulty = results_df.groupby('Dataset').agg({\n",
    "            'Correlation': 'mean',\n",
    "            'Compression Ratio': 'mean'\n",
    "        }).sort_values('Correlation', ascending=False)\n",
    "        \n",
    "        print(\"Dataset Ranking (by average reconstruction quality):\")\n",
    "        for i, (dataset, row) in enumerate(dataset_difficulty.iterrows(), 1):\n",
    "            print(f\"  {i}. {dataset}: Avg Correlation = {row['Correlation']:.3f}, Avg Ratio = {row['Compression Ratio']:.2f}x\")\n",
    "        \n",
    "        print(\"\\n💡 RECOMMENDATIONS:\")\n",
    "        print(\"-\" * 18)\n",
    "        \n",
    "        # Method-specific recommendations\n",
    "        if 'IFS' in results_df['Method'].values:\n",
    "            ifs_data = results_df[results_df['Method'] == 'IFS']\n",
    "            print(f\"\\n🔄 IFS Compression:\")\n",
    "            print(f\"  - Best for: {ifs_data.loc[ifs_data['Correlation'].idxmax(), 'Dataset']} datasets\")\n",
    "            print(f\"  - Average compression: {ifs_data['Compression Ratio'].mean():.2f}x\")\n",
    "            print(f\"  - Use when: Signal has global self-similarity\")\n",
    "            print(f\"  - Avoid when: Signal is very noisy or has local variations\")\n",
    "        \n",
    "        if 'Fractal Coding' in results_df['Method'].values:\n",
    "            fc_data = results_df[results_df['Method'] == 'Fractal Coding']\n",
    "            print(f\"\\n🔄 Fractal Coding:\")\n",
    "            print(f\"  - Best for: {fc_data.loc[fc_data['Correlation'].idxmax(), 'Dataset']} datasets\")\n",
    "            print(f\"  - Average compression: {fc_data['Compression Ratio'].mean():.2f}x\")\n",
    "            print(f\"  - Use when: Signal has local self-similarity\")\n",
    "            print(f\"  - Avoid when: Signal is completely random\")\n",
    "        \n",
    "        print(\"\\n🎯 GENERAL GUIDELINES:\")\n",
    "        print(\"  1. For smooth, periodic signals: Use IFS with 3-4 transformations\")\n",
    "        print(\"  2. For complex, multi-scale signals: Use Fractal Coding with block size 8-12\")\n",
    "        print(\"  3. For real-time applications: Consider processing time trade-offs\")\n",
    "        print(\"  4. For high-fidelity reconstruction: Combine methods with Fractal Interpolation\")\n",
    "        \n",
    "        # Fractal properties summary\n",
    "        if fractal_df is not None and not fractal_df.empty:\n",
    "            print(\"\\n🌀 FRACTAL PROPERTIES PRESERVATION:\")\n",
    "            print(\"-\" * 40)\n",
    "            \n",
    "            fractal_summary = fractal_df.groupby('Method').agg({\n",
    "                'FD Error': 'mean',\n",
    "                'Hurst Error': 'mean'\n",
    "            })\n",
    "            \n",
    "            for method, row in fractal_summary.iterrows():\n",
    "                print(f\"  {method}:\")\n",
    "                print(f\"    Fractal Dimension Error: {row['FD Error']:.4f}\")\n",
    "                print(f\"    Hurst Exponent Error: {row['Hurst Error']:.4f}\")\n",
    "        \n",
    "        print(\"\\n⚠️  LIMITATIONS:\")\n",
    "        print(\"-\" * 15)\n",
    "        print(\"  • Compression ratios lower than specialized algorithms (GZIP, LZ77)\")\n",
    "        print(\"  • Higher computational cost for optimization\")\n",
    "        print(\"  • Performance depends on signal's fractal properties\")\n",
    "        print(\"  • Parameter tuning required for optimal results\")\n",
    "        \n",
    "        print(\"\\n🚀 FUTURE WORK:\")\n",
    "        print(\"-\" * 15)\n",
    "        print(\"  • Adaptive parameter selection\")\n",
    "        print(\"  • Hybrid compression methods\")\n",
    "        print(\"  • GPU acceleration for optimization\")\n",
    "        print(\"  • Real-world dataset evaluation\")\n",
    "        \n",
    "    else:\n",
    "        print(\"❌ No results available for summary\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"🎉 Analysis Complete! Thank you for exploring fractal compression!\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "# Generate final summary\n",
    "generate_final_summary(results_df, fractal_df if 'fractal_df' in locals() else None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Results\n",
    "\n",
    "Finally, let's save our results for future reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to files\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Create results directory\n",
    "results_dir = 'notebook_results'\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Save results DataFrame\n",
    "if not results_df.empty:\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    # Save to CSV\n",
    "    csv_path = os.path.join(results_dir, f'compression_results_{timestamp}.csv')\n",
    "    results_df.to_csv(csv_path, index=False)\n",
    "    print(f\"✅ Results saved to: {csv_path}\")\n",
    "    \n",
    "    # Save fractal results if available\n",
    "    if 'fractal_df' in locals() and fractal_df is not None:\n",
    "        fractal_csv_path = os.path.join(results_dir, f'fractal_results_{timestamp}.csv')\n",
    "        fractal_df.to_csv(fractal_csv_path, index=False)\n",
    "        print(f\"✅ Fractal results saved to: {fractal_csv_path}\")\n",
    "    \n",
    "    # Save parameter sensitivity results\n",
    "    if ifs_param_results:\n",
    "        import json\n",
    "        param_path = os.path.join(results_dir, f'parameter_sensitivity_{timestamp}.json')\n",
    "        with open(param_path, 'w') as f:\n",
    "            json.dump({\n",
    "                'ifs_parameters': ifs_param_results,\n",
    "                'fractal_coding_parameters': fc_param_results\n",
    "            }, f, indent=2)\n",
    "        print(f\"✅ Parameter sensitivity results saved to: {param_path}\")\n",
    "    \n",
    "    print(f\"\\n📁 All results saved in directory: {results_dir}/\")\n",
    "    print(\"\\n🎊 Notebook execution completed successfully!\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ No results to save\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}